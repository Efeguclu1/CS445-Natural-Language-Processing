{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TXP5nalr9ls"
   },
   "source": [
    "# **My Tokenizer**\n",
    "\n",
    "In this assignment, you are asked to create your own word tokenizer without the help of external tokenizers. Steps to the assignment:\n",
    "1. Choose one of the corpora from nltk.corpus list given - assign it to corpus_name\n",
    "1. Create your tokenizer in the code block - tokenize the selected corpus into token_list\n",
    "1. Give the raw corpus text, corpus_raw, and the my_token_list to the evaluation block\n",
    "\n",
    "Only splitting on whitespace is not enough. At least try two other improvements on the tokenization. Please write sufficient comments to show your reasoning.\n",
    "\n",
    "## Rules\n",
    "### Allowed:\n",
    " - Choosing a top-down tokenizer or bottom-up tokenizer\n",
    " - Using regular expressions library (import re)\n",
    " - Adding additional coding blocks\n",
    " - Having an additional dataset if you are creating a bottom-up tokenizer but you need to be able to run the code standalone.\n",
    "\n",
    "### Not allowed:\n",
    " - Using tokenizer libraries such as nltk.tokenize, or any other external libraries to tokenize.\n",
    " - Changing the contents of the evaluation block at the end of the notebook.\n",
    "\n",
    "## Assignment Report\n",
    "Please write a short assignment report at the end of the notebook (max 500 words). Please include all of the following points in the report:\n",
    " - Corpus name and the selection reason\n",
    " - Design of the tokenizer and reasoning\n",
    " - Challenges you have faced while writing the tokenizer and challenges with the specific corpus\n",
    " - Limitations of your approach\n",
    " - Possible improvements to the system\n",
    "\n",
    "## Grading\n",
    "You will be graded with the following criteria:\n",
    " - running complete code (0.5),\n",
    " - tokenizer algorithm (2),\n",
    " - clear commenting (0.5),\n",
    " - evaluation score - comparison with nltk word tokenizer (at most 1 point),\n",
    " - assignment report (1).\n",
    "\n",
    "## Submission\n",
    "\n",
    "Submission will be made to SUCourse. Please submit your file using the following naming convention.\n",
    "\n",
    "\n",
    "`studentid_studentname_tokenizer.ipynb Â - ex. 26744_aysegulrana_tokenizer.ipynb`\n",
    "\n",
    "\n",
    "**Deadline is October 22nd, 5pm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle possessives\n",
    "def handle_possessives(text):\n",
    "    # Replace possessives like \"John's\" with \"John 's\"\n",
    "    text = re.sub(r\"(\\w+)'s\", r\"\\1 's\", text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-jYyH_qz3Lii"
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(corpus_raw):\n",
    "\n",
    "    corpus_raw= corpus_raw.lower()\n",
    "\n",
    "\n",
    "    contractions = {\n",
    "        \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"who'd\": \"who would\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    type corpus_raw: string\n",
    "    param corpus_raw: The raw output of the corpus to be tokenized\n",
    "    rtype: list\n",
    "    return: a list of tokens extracted from the corpus_raw\n",
    "    '''\n",
    "    token_list =corpus_raw.split()\n",
    "    \"\"\"\n",
    "    \n",
    "    for contraction, expansion in contractions.items():\n",
    "        corpus_raw = re.sub(r'\\b' + contraction + r'\\b', expansion, corpus_raw)\n",
    "    corpus_raw = handle_possessives(corpus_raw)\n",
    "\n",
    "    # This regex matches words, numbers, and punctuation\n",
    "    token_list = re.findall(r\"\\b\\w+\\b|[.,!?;:()]\", corpus_raw)\n",
    "\n",
    "    # Keep hyphenated words together (e.g., \"well-being\"), but split compound hyphens (e.g., \"mother-in-law\")\n",
    "    token_list = [re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', token) for token in token_list]\n",
    "\n",
    "    \n",
    "    # write your tokenizer here and apply to corpus_raw. Return the resulting token_list.\n",
    "    # you are NOT allowed to use external tokenizers such as word_tokenize from nltk.\n",
    "    # Only splitting on whitespace is not enough. At least try two other improvements on the tokenization.\n",
    "\n",
    "\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HP0awlu3jci"
   },
   "source": [
    "You are allowed to add code blocks above to use for your tokenizer or evaluate it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-ZF4WJjKxZfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her', 'mother', 'had', 'died', 'too']\n"
     ]
    }
   ],
   "source": [
    "#main code to run your tokenizer.\n",
    "\n",
    "#import your libraries here\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "\n",
    "#select the corpus name from the list below\n",
    "#gutenberg, webtext, reuters, product_reviews_2\n",
    "\n",
    "corpus_name = 'gutenberg'\n",
    "\n",
    "#download the corpus and import it.\n",
    "corpus_raw=\"\"\n",
    "for file in gutenberg.fileids():\n",
    "    corpus_raw += gutenberg.raw(file)\n",
    "\n",
    "#get the raw text output of the corpus to the corpus_raw variable.\n",
    "\n",
    "#call your tokenizer method\n",
    "my_tokenized_list = my_tokenizer(corpus_raw)\n",
    "\n",
    "print(my_tokenized_list[:100])  # Display the first 50 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iuTDStwY36cT"
   },
   "source": [
    "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8txzw8Ag8ysD"
   },
   "outputs": [],
   "source": [
    "def similarity_score(set_a, set_b):\n",
    "    '''\n",
    "    type set_a: set\n",
    "    param set_a: The first set to be compared\n",
    "    type set_b: set\n",
    "    param set_b: The tokens extracted from the corpus_raw\n",
    "    rtype: float\n",
    "    return: similarity score with two sets using Jaccard similarity.\n",
    "    '''\n",
    "\n",
    "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
    "\n",
    "    return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6wUTqReb36Hg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/efeguclu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk import punkt\n",
    "\n",
    "def evaluation(corpus_raw, token_list):\n",
    "    '''\n",
    "    type corpus_raw: string\n",
    "    param corpus_raw: The raw output of the corpus\n",
    "    type token_list: list\n",
    "    param token_list: The tokens extracted from the corpus_raw\n",
    "    rtype: float\n",
    "    return: comparison score with the given token list and the nltk tokenizer.\n",
    "    '''\n",
    "\n",
    "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
    "    #we assume case folding is already applied to the token_list\n",
    "    corpus_raw = corpus_raw.lower()\n",
    "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
    "\n",
    "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6Vt_uSBF-NHm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score is 0.77\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
    "\n",
    "print('The similarity score is {:.2f}'.format(eval_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keZ3UBqh4hgP"
   },
   "source": [
    "Please write your report below using clear headlines with markdown syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORPUS NAME AND SELECTION REASON\n",
    "#For this homework, I chose the Gutenberg Corpus. Gutenberg corpus is one of the famous collections of classical texts and presents unique challenges regarding language archaicness, sentence structure complexity, and heavy use of contractions.\n",
    "\n",
    "#DESIGN OF TOKENIZER AND REASONING\n",
    "#All text is in lowercase to ensure uniformity with similarity checker and prevent token mismatch due to capitalization.\n",
    "#Contractions are expanded to their full forms; for instance, don't is changed to do not in order to avoid splits and make it possible to handle them as one word.\n",
    "#This step is necessary for the tokenizer to align with NLTK when handling words. For example, possessive \"John's\" would be tokenized into two tokens - base word and possessive marker.\n",
    "#This would align with how it is treated by NLTK. Tokenizer treats punctuation as if it were separate tokens since this allows for fine grained tokenization. \n",
    "#Hyphenated words are separated into individual tokens to avoid word combining; this could be treated as separate words by nltk\n",
    "#Tokenizer accounts for extra spaces and new lines to make everything consistent.\n",
    "\n",
    "#CHALLENGES FACED\n",
    "#This mainly involved making the tokenizer NLTK-like, especially with regard to punctuation and contractions. \n",
    "#Archaic language necessitated careful handling of contractions, many of which differ from the conventions of modern usage. Also, balancing the treatment of punctuation-where some should be separated and others kept within tokens-proved complex. \n",
    "#his required extra rules for possessives and abbreviations, such that words like \"it's\" and \"Dr.\" would be tokenized correctly.\n",
    "#The other challenge is tokenizing the entire corpus. In dealing with such a huge volume of text, there could be several performance concerns that may include missed edge cases.\n",
    "\n",
    "#LIMITATIONS OF APPROACH\n",
    "#he limitation of the approach at this stage consists in the fact that certain edge cases are not handled, like complex abbreviation and hyphenated phrases, which NLTK might do. \n",
    "#While the present tokenizer does a good job of separating punctuation, it sometimes does not match NLTK's treatment of punctuation in context, especially quotes and dashes.\n",
    "\n",
    "#POSSIBLE IMPROVMENTS\n",
    "#Advanced regular expressions can handle more complex abbreviations, decimals, and special punctuation. The more context-sensitive rules are added, the better the edge cases will be handled by the tokenizer.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
